{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "header",
   "metadata": {},
   "source": [
    "# RAG Retrieval Evaluation with Golden Chunks\n",
    "\n",
    "This notebook evaluates chunk-level retrieval quality against a golden dataset of 25 queries with ground-truth chunk IDs.\n",
    "\n",
    "**Pipeline:** embed query → hybrid retrieval (FAISS + BM25 with RRF) → cross-encoder reranking → compare against golden chunks\n",
    "\n",
    "**Prerequisites:**\n",
    "- Run `python ingest.py` to build the vectorstore\n",
    "- Run `python eval/generate_chunk_labels.py` to ensure chunk IDs are current\n",
    "- Ollama running at `localhost:11434` (only needed for Section 7: generation)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "sec1-header",
   "metadata": {},
   "source": [
    "## 1. Setup & Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "setup",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Repo root: /Users/dennispudzha/github_projects/rag_science\n",
      "Chunk size: 500 tokens, overlap: 50\n",
      "Retrieval: TOP_K=4, candidates=50\n",
      "Embedding: nomic-embed-text\n",
      "Reranker: BAAI/bge-reranker-v2-m3\n",
      "LLM: llama3.1:8b\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import json\n",
    "from pathlib import Path\n",
    "\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Resolve repo root\n",
    "REPO_ROOT = Path.cwd().resolve()\n",
    "if not (REPO_ROOT / \"config.py\").exists():\n",
    "    REPO_ROOT = REPO_ROOT.parent\n",
    "assert (REPO_ROOT / \"config.py\").exists(), f\"Cannot find repo root (tried {REPO_ROOT})\"\n",
    "\n",
    "if str(REPO_ROOT) not in sys.path:\n",
    "    sys.path.insert(0, str(REPO_ROOT))\n",
    "\n",
    "from config import CHUNK_SIZE, CHUNK_OVERLAP, TOP_K, TOP_K_CANDIDATES, LLM_MODEL, EMBEDDING_MODEL, RERANK_MODEL\n",
    "\n",
    "print(f\"Repo root: {REPO_ROOT}\")\n",
    "print(f\"Chunk size: {CHUNK_SIZE} tokens, overlap: {CHUNK_OVERLAP}\")\n",
    "print(f\"Retrieval: TOP_K={TOP_K}, candidates={TOP_K_CANDIDATES}\")\n",
    "print(f\"Embedding: {EMBEDDING_MODEL}\")\n",
    "print(f\"Reranker: {RERANK_MODEL}\")\n",
    "print(f\"LLM: {LLM_MODEL}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "load-retriever",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "86188d44ac624738a85969f0f5cd2d7e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading weights:   0%|          | 0/393 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning: You are sending unauthenticated requests to the HF Hub. Please set a HF_TOKEN to enable higher rate limits and faster downloads.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FAISS vectors: 5,892\n",
      "BM25 docs: 5,892\n",
      "Weights: dense=0.7, bm25=0.3\n"
     ]
    }
   ],
   "source": [
    "from retriever import load_vectorstore, build_retriever\n",
    "\n",
    "vs = load_vectorstore()\n",
    "retriever = build_retriever(vs)\n",
    "\n",
    "print(f\"FAISS vectors: {vs.index.ntotal:,}\")\n",
    "print(f\"BM25 docs: {len(retriever.bm25_docs):,}\")\n",
    "print(f\"Weights: dense={retriever.dense_weight}, bm25={retriever.bm25_weight}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "sec2-header",
   "metadata": {},
   "source": [
    "## 2. Golden Dataset Overview"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "load-golden",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Queries: 25\n",
      "Fields: ['question', 'ideal_answer', 'expected_sources', 'expected_pages', 'expected_chunk_ids']\n",
      "Golden chunks per query: min=1, max=3, mean=2.4\n"
     ]
    }
   ],
   "source": [
    "golden_path = REPO_ROOT / \"eval\" / \"golden_dataset.json\"\n",
    "golden_dataset = json.loads(golden_path.read_text())\n",
    "\n",
    "print(f\"Queries: {len(golden_dataset)}\")\n",
    "print(f\"Fields: {list(golden_dataset[0].keys())}\")\n",
    "\n",
    "# Show how many golden chunks per query\n",
    "chunk_counts = [len(q.get(\"expected_chunk_ids\", [])) for q in golden_dataset]\n",
    "print(f\"Golden chunks per query: min={min(chunk_counts)}, max={max(chunk_counts)}, mean={sum(chunk_counts)/len(chunk_counts):.1f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "show-samples",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>question</th>\n",
       "      <th>ideal_answer</th>\n",
       "      <th>expected_chunk_ids</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>What CMOS technology node is the ITkPixV2 chip...</td>\n",
       "      <td>ITkPixV2 is fabricated in 65nm CMOS technology.</td>\n",
       "      <td>[2502.05097v1.pdf|p2|23b357c95116, 1_ATLAS ITk...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>What is the pixel size of ITkPixV2?</td>\n",
       "      <td>ITkPixV2 has a pixel size of 50 by 50 micromet...</td>\n",
       "      <td>[2502.05097v1.pdf|p2|23b357c95116, rd53cATLAS1...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>How many modules make up the ITk Pixel detecto...</td>\n",
       "      <td>The ITk Pixel detector consists of 9716 module...</td>\n",
       "      <td>[6_ATLAS ITk pixel detector overview.pdf|p1|23...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>What is the trigger rate requirement for the I...</td>\n",
       "      <td>The trigger rate requirement for ITkPixV2 is 1...</td>\n",
       "      <td>[2502.05097v1.pdf|p2|23b357c95116, 6_ATLAS ITk...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>What are the specifications of the FELIX FLX-7...</td>\n",
       "      <td>The FLX-712 is a PCIe card with a 16-lane PCIe...</td>\n",
       "      <td>[2_FELIX_the_Detector_Interface_for_the_ATLAS_...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            question  \\\n",
       "0  What CMOS technology node is the ITkPixV2 chip...   \n",
       "1                What is the pixel size of ITkPixV2?   \n",
       "2  How many modules make up the ITk Pixel detecto...   \n",
       "3  What is the trigger rate requirement for the I...   \n",
       "4  What are the specifications of the FELIX FLX-7...   \n",
       "\n",
       "                                        ideal_answer  \\\n",
       "0    ITkPixV2 is fabricated in 65nm CMOS technology.   \n",
       "1  ITkPixV2 has a pixel size of 50 by 50 micromet...   \n",
       "2  The ITk Pixel detector consists of 9716 module...   \n",
       "3  The trigger rate requirement for ITkPixV2 is 1...   \n",
       "4  The FLX-712 is a PCIe card with a 16-lane PCIe...   \n",
       "\n",
       "                                  expected_chunk_ids  \n",
       "0  [2502.05097v1.pdf|p2|23b357c95116, 1_ATLAS ITk...  \n",
       "1  [2502.05097v1.pdf|p2|23b357c95116, rd53cATLAS1...  \n",
       "2  [6_ATLAS ITk pixel detector overview.pdf|p1|23...  \n",
       "3  [2502.05097v1.pdf|p2|23b357c95116, 6_ATLAS ITk...  \n",
       "4  [2_FELIX_the_Detector_Interface_for_the_ATLAS_...  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Display first 5 entries\n",
    "df_golden = pd.DataFrame(golden_dataset)\n",
    "df_golden[[\"question\", \"ideal_answer\", \"expected_chunk_ids\"]].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "sec3-header",
   "metadata": {},
   "source": [
    "## 3. Single Query Demo\n",
    "\n",
    "Walk through one query end-to-end: retrieve chunks, inspect content, compare against golden chunks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "single-query",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question: What CMOS technology node is the ITkPixV2 chip fabricated in?\n",
      "Golden answer: ITkPixV2 is fabricated in 65nm CMOS technology.\n",
      "\n",
      "Expected chunks (3):\n",
      "  2502.05097v1.pdf|p2|23b357c95116\n",
      "  1_ATLAS ITk Pixel Detector Overview.pdf|p3|fd78f7da6afe\n",
      "  2502.05097v1.pdf|p2|4fa71090f2d2\n",
      "\n",
      "Retrieved chunks (4):\n",
      "  [miss ] 2502.05097v1.pdf|p1|c6811c3baa8f\n",
      "          [Paper: Prepared for submission to JINST]\n",
      "Prepared for submission to JINST\n",
      "\n",
      "## Page 1\n",
      "\n",
      "Prepared for submission to JINST\n",
      "...\n",
      "  [miss ] Göttingen_RD53B_Seminar_06-21.pdf|p20|0bdcdbefb22b\n",
      "          [Paper: PARTICLE PHYSICS SEMINAR,]\n",
      "large size transistors are basically insensitive\n",
      "\n",
      "to TID\n",
      "\n",
      "−\n",
      "\n",
      "For digital design modif...\n",
      "  [miss ] Mironova_PSD13.pdf|p1|0c2f22d6974b\n",
      "          [Paper: First results from the ATLAS ITkPixV2]\n",
      "First results from the ATLAS ITkPixV2\n",
      "\n",
      "## Page 1\n",
      "\n",
      "First results from the ...\n",
      "  [miss ] 6_ATLAS ITk pixel detector overview.pdf|p3|fdb404f03cd8\n",
      "          [Paper: PoS(ICHEP2020)878] [Section: 3.\n",
      "\n",
      "Pixel modules]\n",
      "as well as the reverse\n",
      "\n",
      "bias for the p-n junction of the sensors...\n"
     ]
    }
   ],
   "source": [
    "from eval.evaluate import doc_chunk_id, extract_retrieved_chunk_ids\n",
    "\n",
    "QUERY_IDX = 0\n",
    "q = golden_dataset[QUERY_IDX]\n",
    "docs = retriever.invoke(q[\"question\"])\n",
    "expected = set(q.get(\"expected_chunk_ids\", []))\n",
    "\n",
    "print(f\"Question: {q['question']}\")\n",
    "print(f\"Golden answer: {q['ideal_answer']}\")\n",
    "print(f\"\\nExpected chunks ({len(expected)}):\")\n",
    "for cid in q.get(\"expected_chunk_ids\", []):\n",
    "    print(f\"  {cid}\")\n",
    "\n",
    "print(f\"\\nRetrieved chunks ({len(docs)}):\")\n",
    "for doc in docs:\n",
    "    cid = doc_chunk_id(doc)\n",
    "    tag = \"MATCH\" if cid in expected else \"miss \"\n",
    "    print(f\"  [{tag}] {cid}\")\n",
    "    print(f\"          {doc.page_content[:120]}...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "sec4-header",
   "metadata": {},
   "source": [
    "## 4. Chunk-Level Retrieval Evaluation\n",
    "\n",
    "For all 25 queries, retrieve top-K chunks and compare against golden chunk IDs.\n",
    "\n",
    "**Metrics:**\n",
    "- **Chunk MRR** — reciprocal rank of the first matching golden chunk\n",
    "- **Chunk Precision@K** — fraction of retrieved chunks that are golden\n",
    "- **Chunk Recall@K** — fraction of golden chunks that were retrieved\n",
    "- **Source MRR / Recall@K** — same metrics at the source (paper) level"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "run-eval",
   "metadata": {},
   "outputs": [],
   "source": [
    "from eval.evaluate import (\n",
    "    reciprocal_rank_chunks,\n",
    "    precision_at_k_chunks,\n",
    "    recall_at_k_chunks,\n",
    "    reciprocal_rank,\n",
    "    recall_at_k,\n",
    ")\n",
    "\n",
    "K = retriever.k\n",
    "rows = []\n",
    "\n",
    "for q in golden_dataset:\n",
    "    expected_chunks = q.get(\"expected_chunk_ids\", [])\n",
    "    expected_sources = q.get(\"expected_sources\", [])\n",
    "    docs = retriever.invoke(q[\"question\"])\n",
    "\n",
    "    rows.append({\n",
    "        \"question\": q[\"question\"],\n",
    "        \"chunk_mrr\": reciprocal_rank_chunks(expected_chunks, docs),\n",
    "        \"chunk_prec\": precision_at_k_chunks(expected_chunks, docs, K),\n",
    "        \"chunk_recall\": recall_at_k_chunks(expected_chunks, docs, K),\n",
    "        \"src_mrr\": reciprocal_rank(expected_sources, docs),\n",
    "        \"src_recall\": recall_at_k(expected_sources, docs, K),\n",
    "    })\n",
    "\n",
    "df_eval = pd.DataFrame(rows)\n",
    "df_eval[[\"question\", \"chunk_mrr\", \"chunk_prec\", \"chunk_recall\", \"src_mrr\", \"src_recall\"]]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "sec5-header",
   "metadata": {},
   "source": [
    "## 5. Aggregate Metrics Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "agg-metrics",
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics = {\n",
    "    \"Chunk MRR\": df_eval[\"chunk_mrr\"].mean(),\n",
    "    f\"Chunk Precision@{K}\": df_eval[\"chunk_prec\"].mean(),\n",
    "    f\"Chunk Recall@{K}\": df_eval[\"chunk_recall\"].mean(),\n",
    "    \"Source MRR\": df_eval[\"src_mrr\"].mean(),\n",
    "    f\"Source Recall@{K}\": df_eval[\"src_recall\"].mean(),\n",
    "}\n",
    "\n",
    "print(f\"{'Metric':<25} {'Value':>8}\")\n",
    "print(\"-\" * 35)\n",
    "for name, val in metrics.items():\n",
    "    print(f\"{name:<25} {val:>8.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bar-chart",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(8, 4))\n",
    "names = list(metrics.keys())\n",
    "values = list(metrics.values())\n",
    "colors = [\"#2196F3\"] * 3 + [\"#4CAF50\"] * 2  # blue for chunk, green for source\n",
    "\n",
    "bars = ax.barh(names, values, color=colors)\n",
    "ax.set_xlim(0, 1)\n",
    "ax.set_xlabel(\"Score\")\n",
    "ax.set_title(\"Retrieval Evaluation Metrics\")\n",
    "\n",
    "for bar, val in zip(bars, values):\n",
    "    ax.text(bar.get_width() + 0.02, bar.get_y() + bar.get_height() / 2,\n",
    "            f\"{val:.3f}\", va=\"center\", fontsize=10)\n",
    "\n",
    "ax.invert_yaxis()\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "sec6-header",
   "metadata": {},
   "source": [
    "## 6. Error Analysis\n",
    "\n",
    "Inspect queries with the worst chunk recall — where did retrieval fail?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "error-analysis",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sort by chunk recall (ascending) to show worst performers first\n",
    "df_worst = df_eval.sort_values(\"chunk_recall\").head(5)\n",
    "\n",
    "for _, row in df_worst.iterrows():\n",
    "    q = next(q for q in golden_dataset if q[\"question\"] == row[\"question\"])\n",
    "    docs = retriever.invoke(q[\"question\"])\n",
    "    retrieved_ids = extract_retrieved_chunk_ids(docs)\n",
    "    expected_ids = q.get(\"expected_chunk_ids\", [])\n",
    "\n",
    "    print(f\"Q: {q['question'][:80]}\")\n",
    "    print(f\"  Chunk MRR={row['chunk_mrr']:.2f}  Recall={row['chunk_recall']:.2f}  Prec={row['chunk_prec']:.2f}\")\n",
    "    print(f\"  Expected:  {expected_ids[:3]}\")\n",
    "    print(f\"  Retrieved: {retrieved_ids[:3]}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "sec7-header",
   "metadata": {},
   "source": [
    "## 7. End-to-End Generation\n",
    "\n",
    "Run a few queries through the full pipeline (retrieve + generate) and compare generated answers to golden answers.\n",
    "\n",
    "**Requires Ollama running.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "generation",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_ollama import ChatOllama\n",
    "from config import OLLAMA_BASE_URL\n",
    "\n",
    "llm = ChatOllama(model=LLM_MODEL, base_url=OLLAMA_BASE_URL, temperature=0)\n",
    "\n",
    "\n",
    "def generate_answer(question: str, docs) -> str:\n",
    "    \"\"\"Stuff retrieved chunks into a prompt and generate an answer.\"\"\"\n",
    "    context = \"\\n\\n\".join(doc.page_content for doc in docs)\n",
    "    prompt = (\n",
    "        \"Answer the question using only the provided context. \"\n",
    "        \"Be precise and cite specific numbers.\\n\\n\"\n",
    "        f\"Context:\\n{context}\\n\\n\"\n",
    "        f\"Question: {question}\\n\\nAnswer:\"\n",
    "    )\n",
    "    return llm.invoke(prompt).content\n",
    "\n",
    "\n",
    "SAMPLE_N = 5\n",
    "for q in golden_dataset[:SAMPLE_N]:\n",
    "    docs = retriever.invoke(q[\"question\"])\n",
    "    answer = generate_answer(q[\"question\"], docs)\n",
    "    retrieved_ids = extract_retrieved_chunk_ids(docs)\n",
    "    expected_ids = set(q.get(\"expected_chunk_ids\", []))\n",
    "    chunk_hits = sum(1 for cid in retrieved_ids if cid in expected_ids)\n",
    "\n",
    "    print(f\"Q: {q['question']}\")\n",
    "    print(f\"Golden:    {q['ideal_answer'][:150]}\")\n",
    "    print(f\"Generated: {answer[:150]}\")\n",
    "    print(f\"Chunks: {chunk_hits}/{len(expected_ids)} golden chunks retrieved\")\n",
    "    print(\"-\" * 80)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
