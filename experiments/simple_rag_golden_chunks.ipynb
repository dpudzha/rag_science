{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "header",
   "metadata": {},
   "source": [
    "# RAG Retrieval Evaluation with Golden Chunks\n",
    "\n",
    "This notebook evaluates chunk-level retrieval quality against a golden dataset of 25 queries with ground-truth chunk IDs.\n",
    "\n",
    "**Pipeline:** embed query → hybrid retrieval (FAISS + BM25 with RRF) → cross-encoder reranking → compare against golden chunks\n",
    "\n",
    "**Prerequisites:**\n",
    "- Run `python ingest.py` to build the vectorstore\n",
    "- Run `python eval/generate_chunk_labels.py` to ensure chunk IDs are current\n",
    "- Ollama running at `localhost:11434` (only needed for Section 7: generation)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "sec1-header",
   "metadata": {},
   "source": [
    "## 1. Setup & Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "setup",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import json\n",
    "from pathlib import Path\n",
    "\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Resolve repo root\n",
    "REPO_ROOT = Path.cwd().resolve()\n",
    "if not (REPO_ROOT / \"config.py\").exists():\n",
    "    REPO_ROOT = REPO_ROOT.parent\n",
    "assert (REPO_ROOT / \"config.py\").exists(), f\"Cannot find repo root (tried {REPO_ROOT})\"\n",
    "\n",
    "if str(REPO_ROOT) not in sys.path:\n",
    "    sys.path.insert(0, str(REPO_ROOT))\n",
    "\n",
    "from config import CHUNK_SIZE, CHUNK_OVERLAP, TOP_K, TOP_K_CANDIDATES, LLM_MODEL, EMBEDDING_MODEL, RERANK_MODEL\n",
    "\n",
    "print(f\"Repo root: {REPO_ROOT}\")\n",
    "print(f\"Chunk size: {CHUNK_SIZE} tokens, overlap: {CHUNK_OVERLAP}\")\n",
    "print(f\"Retrieval: TOP_K={TOP_K}, candidates={TOP_K_CANDIDATES}\")\n",
    "print(f\"Embedding: {EMBEDDING_MODEL}\")\n",
    "print(f\"Reranker: {RERANK_MODEL}\")\n",
    "print(f\"LLM: {LLM_MODEL}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "load-retriever",
   "metadata": {},
   "outputs": [],
   "source": [
    "from retriever import load_vectorstore, build_retriever\n",
    "\n",
    "vs = load_vectorstore()\n",
    "retriever = build_retriever(vs)\n",
    "\n",
    "print(f\"FAISS vectors: {vs.index.ntotal:,}\")\n",
    "print(f\"BM25 docs: {len(retriever.bm25_docs):,}\")\n",
    "print(f\"Weights: dense={retriever.dense_weight}, bm25={retriever.bm25_weight}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "sec2-header",
   "metadata": {},
   "source": [
    "## 2. Golden Dataset Overview"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "load-golden",
   "metadata": {},
   "outputs": [],
   "source": [
    "golden_path = REPO_ROOT / \"eval\" / \"golden_dataset.json\"\n",
    "golden_dataset = json.loads(golden_path.read_text())\n",
    "\n",
    "print(f\"Queries: {len(golden_dataset)}\")\n",
    "print(f\"Fields: {list(golden_dataset[0].keys())}\")\n",
    "\n",
    "# Show how many golden chunks per query\n",
    "chunk_counts = [len(q.get(\"expected_chunk_ids\", [])) for q in golden_dataset]\n",
    "print(f\"Golden chunks per query: min={min(chunk_counts)}, max={max(chunk_counts)}, mean={sum(chunk_counts)/len(chunk_counts):.1f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "show-samples",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display first 5 entries\n",
    "df_golden = pd.DataFrame(golden_dataset)\n",
    "df_golden[[\"question\", \"ideal_answer\", \"expected_chunk_ids\"]].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "sec3-header",
   "metadata": {},
   "source": [
    "## 3. Single Query Demo\n",
    "\n",
    "Walk through one query end-to-end: retrieve chunks, inspect content, compare against golden chunks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "single-query",
   "metadata": {},
   "outputs": [],
   "source": [
    "from eval.evaluate import doc_chunk_id, extract_retrieved_chunk_ids\n",
    "\n",
    "QUERY_IDX = 0\n",
    "q = golden_dataset[QUERY_IDX]\n",
    "docs = retriever.invoke(q[\"question\"])\n",
    "expected = set(q.get(\"expected_chunk_ids\", []))\n",
    "\n",
    "print(f\"Question: {q['question']}\")\n",
    "print(f\"Golden answer: {q['ideal_answer']}\")\n",
    "print(f\"\\nExpected chunks ({len(expected)}):\")\n",
    "for cid in q.get(\"expected_chunk_ids\", []):\n",
    "    print(f\"  {cid}\")\n",
    "\n",
    "print(f\"\\nRetrieved chunks ({len(docs)}):\")\n",
    "for doc in docs:\n",
    "    cid = doc_chunk_id(doc)\n",
    "    tag = \"MATCH\" if cid in expected else \"miss \"\n",
    "    print(f\"  [{tag}] {cid}\")\n",
    "    print(f\"          {doc.page_content[:120]}...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "sec4-header",
   "metadata": {},
   "source": [
    "## 4. Chunk-Level Retrieval Evaluation\n",
    "\n",
    "For all 25 queries, retrieve top-K chunks and compare against golden chunk IDs.\n",
    "\n",
    "**Metrics:**\n",
    "- **Chunk MRR** — reciprocal rank of the first matching golden chunk\n",
    "- **Chunk Precision@K** — fraction of retrieved chunks that are golden\n",
    "- **Chunk Recall@K** — fraction of golden chunks that were retrieved\n",
    "- **Source MRR / Recall@K** — same metrics at the source (paper) level"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "run-eval",
   "metadata": {},
   "outputs": [],
   "source": [
    "from eval.evaluate import (\n",
    "    reciprocal_rank_chunks,\n",
    "    precision_at_k_chunks,\n",
    "    recall_at_k_chunks,\n",
    "    reciprocal_rank,\n",
    "    recall_at_k,\n",
    ")\n",
    "\n",
    "K = retriever.k\n",
    "rows = []\n",
    "\n",
    "for q in golden_dataset:\n",
    "    expected_chunks = q.get(\"expected_chunk_ids\", [])\n",
    "    expected_sources = q.get(\"expected_sources\", [])\n",
    "    docs = retriever.invoke(q[\"question\"])\n",
    "\n",
    "    rows.append({\n",
    "        \"question\": q[\"question\"],\n",
    "        \"chunk_mrr\": reciprocal_rank_chunks(expected_chunks, docs),\n",
    "        \"chunk_prec\": precision_at_k_chunks(expected_chunks, docs, K),\n",
    "        \"chunk_recall\": recall_at_k_chunks(expected_chunks, docs, K),\n",
    "        \"src_mrr\": reciprocal_rank(expected_sources, docs),\n",
    "        \"src_recall\": recall_at_k(expected_sources, docs, K),\n",
    "    })\n",
    "\n",
    "df_eval = pd.DataFrame(rows)\n",
    "df_eval[[\"question\", \"chunk_mrr\", \"chunk_prec\", \"chunk_recall\", \"src_mrr\", \"src_recall\"]]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "sec5-header",
   "metadata": {},
   "source": [
    "## 5. Aggregate Metrics Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "agg-metrics",
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics = {\n",
    "    \"Chunk MRR\": df_eval[\"chunk_mrr\"].mean(),\n",
    "    f\"Chunk Precision@{K}\": df_eval[\"chunk_prec\"].mean(),\n",
    "    f\"Chunk Recall@{K}\": df_eval[\"chunk_recall\"].mean(),\n",
    "    \"Source MRR\": df_eval[\"src_mrr\"].mean(),\n",
    "    f\"Source Recall@{K}\": df_eval[\"src_recall\"].mean(),\n",
    "}\n",
    "\n",
    "print(f\"{'Metric':<25} {'Value':>8}\")\n",
    "print(\"-\" * 35)\n",
    "for name, val in metrics.items():\n",
    "    print(f\"{name:<25} {val:>8.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bar-chart",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(8, 4))\n",
    "names = list(metrics.keys())\n",
    "values = list(metrics.values())\n",
    "colors = [\"#2196F3\"] * 3 + [\"#4CAF50\"] * 2  # blue for chunk, green for source\n",
    "\n",
    "bars = ax.barh(names, values, color=colors)\n",
    "ax.set_xlim(0, 1)\n",
    "ax.set_xlabel(\"Score\")\n",
    "ax.set_title(\"Retrieval Evaluation Metrics\")\n",
    "\n",
    "for bar, val in zip(bars, values):\n",
    "    ax.text(bar.get_width() + 0.02, bar.get_y() + bar.get_height() / 2,\n",
    "            f\"{val:.3f}\", va=\"center\", fontsize=10)\n",
    "\n",
    "ax.invert_yaxis()\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "sec6-header",
   "metadata": {},
   "source": [
    "## 6. Error Analysis\n",
    "\n",
    "Inspect queries with the worst chunk recall — where did retrieval fail?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "error-analysis",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sort by chunk recall (ascending) to show worst performers first\n",
    "df_worst = df_eval.sort_values(\"chunk_recall\").head(5)\n",
    "\n",
    "for _, row in df_worst.iterrows():\n",
    "    q = next(q for q in golden_dataset if q[\"question\"] == row[\"question\"])\n",
    "    docs = retriever.invoke(q[\"question\"])\n",
    "    retrieved_ids = extract_retrieved_chunk_ids(docs)\n",
    "    expected_ids = q.get(\"expected_chunk_ids\", [])\n",
    "\n",
    "    print(f\"Q: {q['question'][:80]}\")\n",
    "    print(f\"  Chunk MRR={row['chunk_mrr']:.2f}  Recall={row['chunk_recall']:.2f}  Prec={row['chunk_prec']:.2f}\")\n",
    "    print(f\"  Expected:  {expected_ids[:3]}\")\n",
    "    print(f\"  Retrieved: {retrieved_ids[:3]}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "sec7-header",
   "metadata": {},
   "source": [
    "## 7. End-to-End Generation\n",
    "\n",
    "Run a few queries through the full pipeline (retrieve + generate) and compare generated answers to golden answers.\n",
    "\n",
    "**Requires Ollama running.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "generation",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_ollama import ChatOllama\n",
    "from config import OLLAMA_BASE_URL\n",
    "\n",
    "llm = ChatOllama(model=LLM_MODEL, base_url=OLLAMA_BASE_URL, temperature=0)\n",
    "\n",
    "\n",
    "def generate_answer(question: str, docs) -> str:\n",
    "    \"\"\"Stuff retrieved chunks into a prompt and generate an answer.\"\"\"\n",
    "    context = \"\\n\\n\".join(doc.page_content for doc in docs)\n",
    "    prompt = (\n",
    "        \"Answer the question using only the provided context. \"\n",
    "        \"Be precise and cite specific numbers.\\n\\n\"\n",
    "        f\"Context:\\n{context}\\n\\n\"\n",
    "        f\"Question: {question}\\n\\nAnswer:\"\n",
    "    )\n",
    "    return llm.invoke(prompt).content\n",
    "\n",
    "\n",
    "SAMPLE_N = 5\n",
    "for q in golden_dataset[:SAMPLE_N]:\n",
    "    docs = retriever.invoke(q[\"question\"])\n",
    "    answer = generate_answer(q[\"question\"], docs)\n",
    "    retrieved_ids = extract_retrieved_chunk_ids(docs)\n",
    "    expected_ids = set(q.get(\"expected_chunk_ids\", []))\n",
    "    chunk_hits = sum(1 for cid in retrieved_ids if cid in expected_ids)\n",
    "\n",
    "    print(f\"Q: {q['question']}\")\n",
    "    print(f\"Golden:    {q['ideal_answer'][:150]}\")\n",
    "    print(f\"Generated: {answer[:150]}\")\n",
    "    print(f\"Chunks: {chunk_hits}/{len(expected_ids)} golden chunks retrieved\")\n",
    "    print(\"-\" * 80)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
