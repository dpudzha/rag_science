{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "# Simple RAG with Golden Chunks — Real Papers\n\nThis notebook demonstrates a minimal RAG evaluation workflow using the **actual ingested scientific papers** and the **real golden dataset** with ground-truth chunk IDs.\n\n**Evaluation approach:**\n1. Load the real FAISS vectorstore and BM25 index (from `ingest.py`)\n2. Load the golden dataset with `expected_chunk_ids` (ground-truth evidence)\n3. For each query, retrieve chunks and compare against golden chunks\n4. Compute chunk-level Precision@K, Recall@K, and MRR\n5. Run generation with Ollama and display answers alongside sources\n\n**Prerequisites:** Run `python ingest.py` first and ensure Ollama is running."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "import sys\nimport json\nimport hashlib\nfrom pathlib import Path\n\nimport pandas as pd\n\n# Resolve repo root and add to path\nREPO_ROOT = Path.cwd().resolve()\nif not (REPO_ROOT / \"config.py\").exists():\n    REPO_ROOT = REPO_ROOT.parent\nassert (REPO_ROOT / \"config.py\").exists(), f\"Cannot find repo root (tried {REPO_ROOT})\"\n\nif str(REPO_ROOT) not in sys.path:\n    sys.path.insert(0, str(REPO_ROOT))\n\nprint(f\"Repo root: {REPO_ROOT}\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Load golden dataset\ngolden_path = REPO_ROOT / \"eval\" / \"golden_dataset.json\"\ngolden_dataset = json.loads(golden_path.read_text())\n\nprint(f\"Golden dataset: {len(golden_dataset)} queries\")\nprint(f\"Fields per entry: {list(golden_dataset[0].keys())}\")\n\ndf_golden = pd.DataFrame(golden_dataset)\ndf_golden[[\"question\", \"expected_sources\", \"expected_chunk_ids\"]].head()"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## 1) Load Real Vectorstore and Build Retriever\n\nLoad the FAISS index, BM25 index, and cross-encoder — the same components used by `query.py`."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "from query import load_vectorstore, build_retriever\n\nvs = load_vectorstore()\nretriever = build_retriever(vs)\n\n# Show vectorstore stats\nn_vectors = vs.index.ntotal\nprint(f\"FAISS vectors: {n_vectors}\")\nprint(f\"BM25 docs: {len(retriever.bm25_docs)}\")\nprint(f\"TOP_K = {retriever.k}, TOP_K_CANDIDATES = {retriever.k_candidates}\")\nprint(f\"Weights: dense={retriever.dense_weight}, bm25={retriever.bm25_weight}\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## 2) Chunk-Level Retrieval Evaluation\n\nFor each golden query, retrieve chunks and compare against `expected_chunk_ids`. This is the core evaluation — **if retrieval is bad, generation cannot be good.**"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "from eval.evaluate import (\n    doc_chunk_id,\n    extract_retrieved_chunk_ids,\n    reciprocal_rank_chunks,\n    precision_at_k_chunks,\n    recall_at_k_chunks,\n    reciprocal_rank,\n    recall_at_k,\n)\n\nK = retriever.k\nrows = []\n\nfor q in golden_dataset:\n    question = q[\"question\"]\n    expected_chunks = q.get(\"expected_chunk_ids\", [])\n    expected_sources = q.get(\"expected_sources\", [])\n\n    docs = retriever.invoke(question)\n    retrieved_ids = extract_retrieved_chunk_ids(docs)\n    retrieved_sources = [d.metadata.get(\"source\", \"?\") for d in docs]\n\n    chunk_mrr = reciprocal_rank_chunks(expected_chunks, docs)\n    chunk_prec = precision_at_k_chunks(expected_chunks, docs, K)\n    chunk_rec = recall_at_k_chunks(expected_chunks, docs, K)\n    src_mrr = reciprocal_rank(expected_sources, docs)\n    src_recall = recall_at_k(expected_sources, docs, K)\n\n    rows.append({\n        \"question\": question[:70],\n        \"chunk_mrr\": chunk_mrr,\n        f\"chunk_prec@{K}\": chunk_prec,\n        f\"chunk_recall@{K}\": chunk_rec,\n        \"src_mrr\": src_mrr,\n        f\"src_recall@{K}\": src_recall,\n        \"retrieved_chunks\": retrieved_ids[:3],\n        \"expected_chunks\": expected_chunks[:3],\n    })\n\ndf_eval = pd.DataFrame(rows)\ndf_eval"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## 3) Aggregate Metrics Summary"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "metrics = {\n    \"Chunk MRR\": df_eval[\"chunk_mrr\"].mean(),\n    f\"Chunk Precision@{K}\": df_eval[f\"chunk_prec@{K}\"].mean(),\n    f\"Chunk Recall@{K}\": df_eval[f\"chunk_recall@{K}\"].mean(),\n    \"Source MRR\": df_eval[\"src_mrr\"].mean(),\n    f\"Source Recall@{K}\": df_eval[f\"src_recall@{K}\"].mean(),\n}\n\nprint(f\"{'Metric':<25} {'Value':>8}\")\nprint(\"-\" * 35)\nfor name, val in metrics.items():\n    print(f\"{name:<25} {val:>8.3f}\")\n\n# Highlight queries where chunk retrieval completely missed\nmisses = df_eval[df_eval[\"chunk_mrr\"] == 0.0]\nif len(misses) > 0:\n    print(f\"\\nChunk-level misses: {len(misses)}/{len(df_eval)}\")\n    for _, row in misses.iterrows():\n        print(f\"  - {row['question']}\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## 4) Detailed Chunk Comparison for a Single Query\n\nPick one query and inspect exactly which chunks were retrieved vs expected — useful for debugging retrieval misses."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "QUERY_IDX = 0  # Change this to inspect different queries\n\nq = golden_dataset[QUERY_IDX]\ndocs = retriever.invoke(q[\"question\"])\n\nprint(f\"Question: {q['question']}\\n\")\nprint(f\"Expected chunk IDs:\")\nfor cid in q.get(\"expected_chunk_ids\", []):\n    print(f\"  {cid}\")\n\nprint(f\"\\nRetrieved chunk IDs:\")\nfor doc in docs:\n    cid = doc_chunk_id(doc)\n    match = \"MATCH\" if cid in q.get(\"expected_chunk_ids\", []) else \"     \"\n    print(f\"  [{match}] {cid}\")\n    print(f\"          {doc.page_content[:120]}...\")\n    print()"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## 5) End-to-End Generation with Ollama\n\nRun the full pipeline: retrieve chunks, then generate an answer using the local LLM. Compare against the golden `ideal_answer`."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "from langchain_ollama import ChatOllama\nfrom config import LLM_MODEL, OLLAMA_BASE_URL\n\nllm = ChatOllama(model=LLM_MODEL, base_url=OLLAMA_BASE_URL, temperature=0)\n\ndef generate_answer(question: str, docs) -> str:\n    \"\"\"Simple RAG generation: stuff retrieved chunks into a prompt.\"\"\"\n    context = \"\\n\\n\".join(doc.page_content for doc in docs)\n    prompt = (\n        \"Answer the question using only the provided context. \"\n        \"Be precise and cite specific numbers.\\n\\n\"\n        f\"Context:\\n{context}\\n\\n\"\n        f\"Question: {question}\\n\\nAnswer:\"\n    )\n    return llm.invoke(prompt).content\n\n# Run on a few sample queries\nSAMPLE_N = 5\nfor q in golden_dataset[:SAMPLE_N]:\n    docs = retriever.invoke(q[\"question\"])\n    answer = generate_answer(q[\"question\"], docs)\n    retrieved_ids = extract_retrieved_chunk_ids(docs)\n    expected_ids = set(q.get(\"expected_chunk_ids\", []))\n    chunk_hits = sum(1 for cid in retrieved_ids if cid in expected_ids)\n\n    print(f\"Q: {q['question']}\")\n    print(f\"Golden: {q['ideal_answer'][:150]}\")\n    print(f\"Generated: {answer[:150]}\")\n    print(f\"Chunks: {chunk_hits}/{len(expected_ids)} golden chunks retrieved\")\n    print(\"-\" * 80)"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}